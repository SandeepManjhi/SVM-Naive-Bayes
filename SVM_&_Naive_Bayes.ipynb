{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#SVM & NAIVE BAYES"
      ],
      "metadata": {
        "id": "DX6Vt2VVW8w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "  - Answer: A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It aims to find the best hyperplane that separates the data into different classes with the maximum margin.\n",
        "  \n",
        "  How SVM Works\n",
        "    1. Hyperplane Selection: SVM selects the hyperplane that maximizes the margin between classes. The margin is the distance between the hyperplane and the nearest data points from each class.\n",
        "    2. Support Vectors: The data points that lie closest to the hyperplane are called support vectors. These points are crucial in defining the hyperplane and determining the classification boundary.\n",
        "    3. Kernel Trick: SVM uses the kernel trick to handle non-linearly separable data. It maps the data into a higher-dimensional space where it becomes linearly separable, allowing SVM to find a hyperplane that separates the classes.\n",
        "    4. Classification: Once the hyperplane is determined, SVM classifies new data points based on which side of the hyperplane they lie on.\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "  - Answer: The main difference between Hard Margin SVM and Soft Margin SVM lies in how they handle the classification of data points.\n",
        "  \n",
        "  Hard Margin SVM\n",
        "  \n",
        "  •\tStrict Separation: Hard Margin SVM requires that all data points be classified correctly and lie on the correct side of the hyperplane.\n",
        "  \n",
        "  •\tNo Misclassifications: It does not allow for any misclassifications, meaning every data point must be on the right side of the decision boundary.\n",
        "  \n",
        "  •\tSensitive to Outliers: Hard Margin SVM is sensitive to outliers and noise in the data, as a single misclassified point can significantly impact the hyperplane.\n",
        "  \n",
        "  •\tHard Margin SVM focuses solely on maximizing the margin.\n",
        "    \n",
        "  Soft Margin SVM\n",
        "    \n",
        "  •\tAllows Misclassifications: Soft Margin SVM allows for some misclassifications by introducing slack variables that permit data points to lie on the wrong side of the hyperplane.\n",
        "    \n",
        "  •\tTrade-off between Margin and Misclassifications: It finds a balance between maximizing the margin and minimizing the number of misclassifications.\n",
        "     \n",
        "  •\tMore Robust: Soft Margin SVM is more robust to outliers and noise, as it can tolerate some errors in classification.\n",
        "    \n",
        "  •\tSoft Margin SVM balances margin maximization with minimizing misclassifications.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "  - Answer: The Kernel Trick is a technique used in Support Vector Machines (SVMs) to handle non-linearly separable data. Instead of explicitly mapping the data into a higher-dimensional space, the Kernel Trick uses a kernel function to compute the dot product of the data points in the higher-dimensional space. This allows SVMs to operate in the higher-dimensional space without explicitly transforming the data.\n",
        "\n",
        "  Example: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "  One popular kernel function is the Radial Basis Function (RBF) kernel, also known as the Gaussian kernel. It is defined as:\n",
        "\n",
        "  K (x, y) = exp (-γ ||x - y||^2)\n",
        "\n",
        "  where γ is a hyperparameter that controls the width of the kernel.\n",
        "\n",
        "  Use Case: Non-Linear Classification\n",
        "\n",
        "  The RBF kernel is particularly useful when the data is not linearly separable in the original feature space. By using the RBF kernel, SVM can map the data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "  For example, consider a dataset with two features (x1, x2) and two classes (blue and red). The data points are distributed in a circular pattern, making it non-linearly separable.\n",
        "\n",
        "        | x1 | x2 | Class |\n",
        "        | -- | -- | ----- |\n",
        "        | 1 | 1 | Blue |\n",
        "        | 1 | -1 | Blue |\n",
        "        | -1 | 1 | Blue |\n",
        "        | -1 | -1 | Red |\n",
        "        | 0.5 | 0.5 | Blue |\n",
        "        | 0.5 | -0.5 | Blue |\n",
        "\n",
        "  Using the RBF kernel, SVM can map the data into a higher-dimensional space where the classes become linearly separable. The SVM model can then learn a decision boundary that separates the classes effectively.\n",
        "\n",
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "  - Answer: A Naïve Bayes Classifier is a type of supervised learning algorithm based on Bayes' theorem. It is used for classification tasks, where the goal is to predict the class label of a new instance based on its features.\n",
        "  \n",
        "  The Naïve Bayes Classifier is called \"naïve\" because it makes a strong assumption about the independence of features. Specifically, it assumes that:\n",
        "    \n",
        "    - Features are conditionally independent: Given the class label, the features are independent of each other.\n",
        "    \n",
        "    - No correlation between features: The presence or absence of one feature does not affect the presence or absence of another feature.\n",
        "\n",
        "  This assumption is often violated in real-world data, where features can be correlated or dependent on each other. Despite this, Naïve Bayes Classifiers can still perform well in many cases, especially when the number of features is large.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "  - Answer: The Naïve Bayes algorithm has several variants, each suited for different types of data distributions. The three main variants are Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "  \n",
        "  Gaussian Naïve Bayes\\\n",
        "      •\tAssumes Continuous Data: Gaussian Naïve Bayes assumes that the features follow a Gaussian (normal) distribution.\\\n",
        "      •\tUsed for Continuous Features: It is suitable for datasets with continuous features, such as numeric values.\n",
        "      \n",
        "      Example Use Cases:\\\n",
        "      •\tPredicting continuous outcomes, like predicting house prices based on features like area and number of rooms.\\\n",
        "      •\tClassifying data with continuous features, such as image classification based on pixel values.\n",
        "\n",
        "  Multinomial Naïve Bayes\\\n",
        "      •\tAssumes Discrete Counts: Multinomial Naïve Bayes is suitable for features that represent discrete counts, such as word frequencies in text data.\\\n",
        "      •\tUsed for Text Classification: It is commonly used in text classification tasks, such as spam detection and sentiment analysis.\n",
        "    \n",
        "      Example Use Cases:\\\n",
        "      •\tClassifying text documents into categories like spam or not spam based on word frequencies.\\\n",
        "      •\tSentiment analysis of customer reviews to determine positive or negative sentiment.\n",
        "\n",
        "  Bernoulli Naïve Bayes\\\n",
        "      •\tAssumes Binary Features: Bernoulli Naïve Bayes is suitable for binary features, where each feature is either present or absent.\\\n",
        "      •\tUsed for Binary Features: It is commonly used in applications where features are represented as binary vectors.\n",
        "      \n",
        "      Example Use Cases:\\\n",
        "      •\tText classification with binary features, such as presence or absence of specific words.\\\n",
        "      •\tDocument classification based on binary features like keyword presence.\n",
        "\n",
        "  The choice of Naïve Bayes variant depends on the nature of the data and the problem at hand:\\\n",
        "      •\tGaussian Naïve Bayes: Use for continuous features with a Gaussian distribution.\\\n",
        "      •\tMultinomial Naïve Bayes: Use for discrete count data, such as word frequencies in text.\\\n",
        "      •\tBernoulli Naïve Bayes: Use for binary features, where each feature is either present or absent.\n",
        "\n"
      ],
      "metadata": {
        "id": "S-dyhnUgXESj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# Load the Iris dataset\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "# Print the model's accuracy and support vectors.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "svm_classifier = svm.SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Print the support vectors\n",
        "support_vectors = svm_classifier.support_vectors_\n",
        "print(f\"Number of Support Vectors: {len(support_vectors)}\")\n",
        "print(\"Support Vectors:\")\n",
        "print(support_vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twGJ1PjAXDqD",
        "outputId": "ebdd07e3-1389-4754-82b1-cfe9d304a3f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.000\n",
            "Number of Support Vectors: 25\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# Load the Breast Cancer dataset\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "# Print its classification report including precision, recall, and F1-score.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print (\"Classification Report:\")\n",
        "print (classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iIa6myzaBiE",
        "outputId": "12d72df6-ef37-4303-ea86-d20b9fa87cfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95        43\n",
            "           1       0.96      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "# Print the best hyperparameters and accuracy.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['rbf', 'linear', 'poly']\n",
        "}\n",
        "\n",
        "# Initialize the SVM model and GridSearchCV\n",
        "svm_model = svm.SVC()\n",
        "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_estimator.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifR4W1v6aRfA",
        "outputId": "24d2daf5-dceb-4b30-ffbb-bc4182fae8bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZYngoncWttb",
        "outputId": "4cd78e44-ed2b-41da-fb95-2e2ddeaf4cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.958\n"
          ]
        }
      ],
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).\n",
        "# Print the model's ROC-AUC score for its predictions.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict_proba(X_test_vectorized)\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "lb = LabelBinarizer()\n",
        "y_test_binarized = lb.fit_transform(y_test)\n",
        "roc_auc = roc_auc_score(y_test_binarized, y_pred, multi_class='ovr')\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Imagine you’re working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "# Text with diverse vocabulary\n",
        "# Potential class imbalance (far more legitimate emails than spam)\n",
        "# Some incomplete or missing data\n",
        "# Explain the approach you would take to:\n",
        "# Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "# Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "# Address class imbalance\n",
        "# Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Load dataset (using 20 Newsgroups as a proxy for email classification)\n",
        "categories = ['alt.atheism', 'talk.religion.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Address class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
        "\n",
        "# Print class distribution before and after resampling\n",
        "print(\"Original class distribution:\", Counter(y_train))\n",
        "print(\"Resampled class distribution:\", Counter(y_train_resampled))\n",
        "\n",
        "# Train Naïve Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-I_YsiCazXb",
        "outputId": "465605f3-1b49-47a0-e6fb-2036fab59a23"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution: Counter({np.int64(0): 639, np.int64(1): 502})\n",
            "Resampled class distribution: Counter({np.int64(1): 639, np.int64(0): 639})\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.89      0.80       160\n",
            "           1       0.80      0.57      0.67       126\n",
            "\n",
            "    accuracy                           0.75       286\n",
            "   macro avg       0.76      0.73      0.73       286\n",
            "weighted avg       0.76      0.75      0.74       286\n",
            "\n",
            "F1-score: 0.7400015714622455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach to Email Classification\n",
        "\n",
        "For Preprocessing the Data\n",
        "1. Text Vectorization: Use techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings (e.g., Word2Vec, GloVe) to convert text into numerical vectors that can be processed by machine learning algorithms.\n",
        "2. Handling Missing Data: For missing subject lines or body content, consider imputing with a placeholder or using the available content for classification. For other features, imputation or removal might be necessary based on the extent of missingness.\n",
        "3. Data Cleaning: Remove stop words, punctuation, and special characters to reduce noise and improve model performance.\n",
        "\n",
        "Appropriate Model\n",
        "1. Naïve Bayes: Given the nature of text data and the potential for a large number of features (words), Naïve Bayes can be a strong baseline. It's particularly effective for text classification tasks due to its simplicity and efficiency.\n",
        "2. SVM: Support Vector Machines can also be effective, especially with the right kernel (e.g., linear or RBF). SVMs are robust to high-dimensional data and can find complex decision boundaries.\n",
        "\n",
        "Justification:\n",
        "- Naïve Bayes is often preferred for text classification due to its simplicity, interpretability, and efficiency. It handles high-dimensional data well and is less prone to overfitting.\n",
        "- SVM can be a good alternative if the dataset is not extremely large, and there's a need for more complex decision boundaries. However, it might require more tuning and computational resources.\n",
        "\n",
        "Addressing Class Imbalance\n",
        "1. Resampling Techniques: Use oversampling the minority class (spam), undersampling the majority class (not spam), or synthetic sampling methods like SMOTE (Synthetic Minority Over-sampling Technique).\n",
        "2. Class Weights: Adjust class weights in the model to give more importance to the minority class. Many algorithms, including SVM and some implementations of Naïve Bayes, support class weighting.\n",
        "3. Evaluation Metrics: Focus on metrics that are robust to class imbalance, such as F1-score, precision, recall, and AUC-ROC, rather than accuracy alone.\n",
        "\n",
        "Evaluating the Performance\n",
        "1. Precision: Measures the proportion of true positives among all positive predictions made by the model.\n",
        "2. Recall: Measures the proportion of true positives among all actual positive instances.\n",
        "3. F1-score: The harmonic means of precision and recall, providing a balanced measure of both.\n",
        "4. AUC-ROC: Measures the model's ability to distinguish between classes, with higher values indicating better performance.\n",
        "\n",
        "Business Impact\n",
        "1. Reduced Manual Effort: Automating email classification reduces the need for manual sorting, saving time and resources.\n",
        "2. Improved User Experience: By accurately filtering out spam, users receive fewer unwanted emails, enhancing their experience and productivity.\n",
        "3. Security: Effective spam filtering can reduce the risk of phishing attacks and malware distribution through emails.\n",
        "4. Cost Savings: Reducing the volume of spam can decrease the costs associated with storage, bandwidth, and support.\n"
      ],
      "metadata": {
        "id": "KxDimKfsbMGQ"
      }
    }
  ]
}